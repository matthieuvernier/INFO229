{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 0: Introducción al Web Scraping en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Realizar consultas HTTP en Python: _Requests_\n",
    "\n",
    "[_Requests_](https://2.python-requests.org//es/latest/) es una librería Python que facilita el envio de consultas HTTP/1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.latercera.com/\"\n",
    "\n",
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.headers['Content-Type'])\n",
    "print(r.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial para uso más avanzado de _Requests_:\n",
    "- _Requests_ quickstart [[1]](https://2.python-requests.org//es/latest/user/quickstart.html)\n",
    "- uso avanzado de _Requests_ [[2]](https://2.python-requests.org//es/latest/user/advanced.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraer datos de documentos HTML en Python: _BeautifulSoup4_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) es una librería Python que facilita la extracción de datos en archivos HTML y XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "bs = BeautifulSoup(html_doc, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = bs.find_all('a')\n",
    "\n",
    "for a_tag in a_tags:\n",
    "    print(a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_tag in a_tags:\n",
    "    print(a_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a_tag in a_tags:\n",
    "    print(a_tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tags = bs.find_all('p',attrs = {'class':'title'})\n",
    "for p_tag in p_tags:\n",
    "    print(p_tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extraer enlaces de noticias desde un medio de prensa: _Requests_ y _BeautifulSoup4_\n",
    "\n",
    "**Nota Bene**: Multiplicar de forma automática las consultas HTTP hacia un sitio web es una mala práctica que no respeta las condiciones de uso del sitio. En ciertos paises, está prohibido por leyes.\n",
    "Para evitar abusar las condiciones de uso de los sitios, cuidaremos no enviar más de 2 consultas por minuto hacia el mismo sitio.\n",
    "\n",
    "Para evitar abusos, cada grupo de estudiantes utilizaran sitios de medios de prensa distintos:\n",
    "- La Tercera: https://www.latercera.com/\n",
    "- Cooperativa: https://www.cooperativa.cl/\n",
    "- Bio Bio: https://www.biobiochile.cl/\n",
    "- CNN Chile: https://www.cnnchile.com/\n",
    "- 24 Horas TVN: https://www.24horas.cl/\n",
    "- El País (España): https://elpais.com/elpais/portada_america.html\n",
    "- El Mundo (españa): https://www.elmundo.es/\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.elmundo.es/ultimas-noticias.html\"\n",
    "\n",
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "a_tags = bs.find_all('a')\n",
    "\n",
    "for a_tag in a_tags:\n",
    "    print(a_tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_tags = bs.find_all('h2', attrs={\"class\":\"mod-title\"})\n",
    "\n",
    "enlaces=[]\n",
    "\n",
    "for h2_tag in h2_tags:\n",
    "    specific_a_tags = h2_tag.find_all('a')\n",
    "    for a_tag in specific_a_tags:\n",
    "        print(a_tag['href'])\n",
    "        enlaces.append(a_tag['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraer contenidos de noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "noticias=[]\n",
    "\n",
    "for enlace in enlaces:\n",
    "    \n",
    "    time.sleep(2)\n",
    "    print(enlace)\n",
    "    r = requests.get(enlace)\n",
    "\n",
    "    bs = BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "    div_noticia = bs.find('div', attrs={'class':'ue-l-article__body ue-c-article__body'})\n",
    "\n",
    "    p_tags = div_noticia.find_all('p')\n",
    "\n",
    "    texto_noticia=\"\"\n",
    "    try:\n",
    "        for p_tag in p_tags:\n",
    "            texto_noticia = texto_noticia+p_tag.text\n",
    "            \n",
    "    except:\n",
    "        continue\n",
    "    noticias.append((enlace,texto_noticia)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aplicar procesos de Tratamiento Automático del Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SpaCy](https://spacy.io) es una librería Python que permite facilitar la realización de ciertos procesos de Tratamiento Automático del Lenguaje (TAL). Ejemplos de procesos de TAL son por ejemplos:\n",
    "- la tokenización: segmentar el texto en palabras,\n",
    "- la clasificación gramatical (pos): identificar si una palabra es un sustantivo, verbo, adjetivo, etc.\n",
    "- la lematización: determinar la forma genérica de una palabra (por ej.: \"superó\" --> \"superar\", \"ocasiones\" --> \"ocasión\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(noticias[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if (token.pos_ == \"NOUN\"):\n",
    "        print(token.text, token.pos_,token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Almacenar los resultados en una base de datos: _MySQL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "db_connection = mysql.connector.connect(user=\"root\",host=\"localhost\",password=\"root\")\n",
    "cursor = db_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EJECUTAR 1 VEZ\n",
    "#cursor.execute(\"CREATE DATABASE info229;\")\n",
    "cursor.execute(\"USE info229\")\n",
    "#cursor.execute(\"CREATE TABLE news (idNews INT PRIMARY KEY NOT NULL AUTO_INCREMENT, url VARCHAR(200), text TEXT);\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noticia in noticias:\n",
    "    query=\"INSERT INTO news(url,text) VALUES (%s,%s)\"\n",
    "    cursor.execute(query,noticia)\n",
    "\n",
    "cursor.execute(\"COMMIT\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
